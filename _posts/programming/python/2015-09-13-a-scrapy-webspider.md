---
layout: article
title: "03 - A Scrapy Webspider"
description: "How to write and use a Scrapy web spider, as well as an example that might be useful to a social scientist."
modified:
categories: programming
subcategories: python
excerpt:
tags: []
image:
  feature:
  teaser: Spider-icon.png
  thumb:
date: 2015-09-13T10:05:22+02:00
---
{% include toc.html %}

How to use Scrapy.  This is basically a summary of the [Scrapy tutorial](http://doc.scrapy.org/en/1.0/intro/tutorial.html){:target="_blank_"}.  As an example, we'll make a web crawler to scrape the [Sciences Po Economics Department website](econ.sciences-po.fr){:target='_blank_'} for seminar information.

Creating a Project
------------------
Scrapy automatically generates a directory containing a set of files you will need.  Simply open a terminal, navigate to the directory where you'd like to store your crawler, and type

{% highlight bash %}
scrapy startproject PiPoCrawl
{% endhighlight %}

That's it!  You now have a full Scrapy project.  The project folder contains several files that we will need to use.  The `items.py` file is where you define what you would like to find.  The `pipelines.py` is for managing output.  The `settings.py` file is pretty self explanatory and the `spiders` folder is where you will store all of your creepy crawlies.

Defining Items to Extract
------------------------
Items work like Python dictionaries.  It is actually possible to use dictionaries, but Scrapy prefers Items, as they have apparently optimized their program for these.  You define the Item class yourself, specifying what information you would like to have output from the spider.  The Scrapy Item class must be defined withing the `items.py` file.  So edit that file.  For now, our `items.py` file looks like the following:

{% highlight python %}
import scrapy

class PipocrawlItem(scrapy.Item):
    date = scrapy.Field()
    title = scrapy.Field()
    description = scrapy.Field()
{% endhighlight %}

Note that I'm ommitting the comments that are automatically generated by Scrapy, but you don't have to (this may seem obvious, but keep in mind that not everyone is a programming guru like yourself...).  

Writing a spider
----------------
Although I suffer from arachnophobia, we are going to check out a spider...  A spider is a python class that defines how we would like to perform our scraping.  They tell scrapy how to crawl (eg follow links), how to extract the data, etc.

I've created a spider called `pipo_spider.py` that scrapes the home page of the Sciences Po Economics Department for seminar information.  The script looks like the following:

{% highlight python %}
import scrapy
from PiPoCrawl.items import PipocrawlItem


class PipocrawlSpider(scrapy.Spider):
    name = "pipo"
    allowed_domains = ["econ.sciences-po.fr"]
    start_urls = [
        "http://econ.sciences-po.fr/"
    ]

    def parse(self, response):
        events = response.selector.xpath("//div[@class='my_agenda']")
        for seminar in events:
            #NOTE: You need to include the period in the xpath expression to
            # make it relative.  Otherwise, you will start from the begining
            #NOTE: Including the text() specification in the xpath drops the
            # html surrounding the content
            item = PipocrawlItem()
            item['date'] = seminar.xpath(".//strong/text()").extract_first()
            #NOTE: Notice the nested selector to get the link text
            item['title'] = seminar.xpath(".//span[@class='titre']")
            item['title'] = item['title'].xpath(".//a/text()").extract()
            item['description'] = seminar.xpath(".//span[@class='desc']/text()"
                                                ).extract()
            yield item

{% endhighlight %}

Let's look at the parts of this file individually.  First, you have to import your custom Items class, using `from PiPoCrawl.items import PipocrawlItem`.  Next, you define your spider class and give it some attributes:

* `name` defines the name of your spider so you can call it from the command line.
* `allowed_domains` defines the site within which your spider is allowed to crawl.  Here, we limit it to the Economics Department's website.
* `start_urls` tels the spider some starting points, as many as you would like to provide.

Now, we can define a `parse()` method within our spider class to process the webiste content.  Here you have the liberty to do as you please and I found that the most difficult step in this process was identifying what you want and how to differentiate it.  For this case, I noticed that all of the seminar entries are defined as `div` objects of a class called `my_agenda`.  Because of this, it is possible to identify all of the individual entries.  So, the first thing to do is to extract all of these entries:

{% highlight python %}
events = response.selector.xpath("//div[@class='my_agenda']")
{% endhighlight %}

This command calls the `response` object, containing the raw html source of the webpage, and applies an xpath selector.  What the heck is that?! [XPath](http://www.w3.org/TR/xpath/){:target='_blank_'} is a "language for addressing parts of an XML document", whatever that means.  If you would like to learn more about XPath and the syntax of these calls, refer to this [New Mexico Tech Reference](http://infohost.nmt.edu/tcc/help/pubs/xslt/xpath-sect.html){:target='_blank_'}, but it is quite dense.  You might be better off just googling specific examples if you don't do much web programming (like me!).

In the example above, the selector looks through the html code and selects all of the instances of `<div> ... <\div>` that are of the class `my_agenda`, placing these into a SelectorsList object.  The nice thing about this object is that you can again use the `xpath` method, nesting text selection.  Anywho, next we loop over the list objects, each one representing a calendar entry.  We have the following loop:

{% highlight python %}
for seminar in events:
    item = PipocrawlItem()
    item['date'] = seminar.xpath(".//strong/text()").extract_first()
    item['title'] = seminar.xpath(".//span[@class='titre']")
    item['title'] = item['title'].xpath(".//a/text()").extract()
    item['description'] = seminar.xpath(".//span[@class='desc']/text()"
                                        ).extract()
    yield item
{% endhighlight %}

For each loop, we define an instance of `PipocrawlItem()` and populate it by scraping the text.  First, take a look at the html of an entry in `events`:

{% highlight html %}
<div id='agenda_4' class='my_agenda'>
	<strong>21-09-2015</strong>
	<span class='titre' id='titre_4'> | <strong><a href="/departmental-seminar-rob-shimer-university-chicago"> Departmental Seminar : Rob SHIMER (University of Chicago)</a></strong></span>
	<br><span class='desc' id='texte_4'> Title: TBA

	The next Departmental Seminar will host Georgy EGOROV (Kellogg School of Management)&nbsp; on September 28th.

	Seminar organized by Elise Huillery and Guillaume Plantin
</span>
</div>
{% endhighlight %}

Notice that the date always comes first and is surrounded by the `<strong>...</strong>` decorator.  So, we can use the `.extract_first()` method.  The syntax within the xpath call is important.  We must include the `.` so that xpath knows we want a relative selector, not an absolute one.  This means we would like to select from a single element of `events`, not from the entire object.  `//strong/text()` specifies the decorator we are looking for and that we want only the text, dropping the `<strong>...</strong>`.

The other two selectors work in the same way, but notice that we select twice for `item['title']`.  This is a nested selection and is needed because the text of the title is also a link.  Finally, yield prints the output, giving us the following:

{% highlight python %}
{'date': u'21-09-2015',
 'description': [u' Title: TBA\r\n\t\r\n\tThe next Departmental Seminar will host Georgy EGOROV (Kellogg School of Management)\xa0 on September 28th.\r\n\t\r\n\tSeminar organized by Elise Huillery and Guillaume Plantin\r\n'],
 'title': [u' Departmental Seminar : Rob SHIMER (University of Chicago)']}
{% endhighlight %}

And that's it!  That is a spider!  The hardest part, for me at least, was figuring out a way to identify the item you would like to scrape.

Writing a Pipeline
-------------------
According to the Scrapy tutorial, the easiest way to store your data is to use a command modifier to export your information to a `.json` file:

{% highlight python %}
scrapy crawl pipo -o items.json
{% endhighlight %}

That is all you need if you simply want to store your scraped data.  However, if you would like to do more to your data before saving it, you might consider a pipeline.  For this example, we'll write a pipeline to clean up the items in our data.  In particular, the description of calendar events has many line break markers (`\r` or `\n`) that just make things look messy.

First, we need to edit our `pipelines.py` script.  Here's how the completed file looks:

{% highlight python %}
class PipocrawlPipeline(object):

    def process_item(self, item, spider):
        replaceable = {u"\r": u"", u"\n": u" ", u"\t": u"" , u"\xa0": u""}
        for i, j in replaceable.items():
            item['date'] = item['date'].replace(i, j)
            item['description'][0] = item['description'][0].replace(i, j)
            item['title'][0] = item['title'][0].replace(i, j)
        return item
{% endhighlight %}

If you know of a better way to do this, so do I!  You can avoid this problem by wrapping the selector in the spider script with another function, but this is just to show how a pipeline works.

This file defines a pipeline class which is run on every item.  What it does is fairly straight forward, looping through the items and replacing any line break, tabs, etc. with emtpy space or a single space.  To make Scrapy use this pipeline, you do not need to add any code to the spider script, simply uncomment the `ITEM_PIPELINES` option in the `settings.py` file and add your pipeline, along with a reference:

{% highlight python %}
ITEM_PIPELINES = {
    'PiPoCrawl.pipelines.PipocrawlPipeline': 300,
}
{% endhighlight %}

The reference number simply tells Scrapy in what order to run your pipelines.  You can do many other things with a pipeline, but you'll have to discover all that for yourself!

Conclusion
------------
Using Scrapy to create web crawlers is fairly straight forward, once you get down the syntax and the file structure.  You can also make your web crawler follow links, but I'll leave that for another post.  If you are interested in this example, I've posted the file folder on GitHub [here](https://github.com/tyler-abbot/PiPoCrawl){:target="_blank_"}.
